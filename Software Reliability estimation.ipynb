{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "AQGLrp1nD0NK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ojp2KIvOeWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2c9d46-efcc-471e-e020-f9cdf254abdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection Rate (Sensitivity): 0.3\n",
            "True Negative Rate (TNR): 0.926829268292683\n",
            "Accuracy: 0.803921568627451\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 7\n",
        "epochs = 100\n",
        "\n",
        "# Initialize and train a multi-layer perceptron (MLP) classifier with batch size and epochs\n",
        "# Adjust parameters as needed (e.g., hidden layer sizes, activation function, solver, etc.)\n",
        "clf = MLPClassifier(hidden_layer_sizes=(50,50,50,50,50,50,50,50,50,50), activation='relu', solver='adam', random_state=42, batch_size=batch_size, max_iter=epochs)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate detection rate (sensitivity)\n",
        "detection_rate = tp / (tp + fn)\n",
        "\n",
        "# Calculate true negative rate (TNR)\n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "# Print detection rate, TNR, and accuracy\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "7lmzl2r_D5CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for CNN input (assuming input shape is (number of samples, number of features, 1))\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Define CNN architecture\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with loading animation\n",
        "num_epochs = 2000\n",
        "with tqdm(total=num_epochs) as pbar:  # Initialize tqdm with total number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit(X_train_reshaped, y_train, epochs=1, batch_size=10, validation_split=0.2, verbose=0)\n",
        "        pbar.update(1)  # Update tqdm progress bar after each epoch\n",
        "\n",
        "# Evaluate model\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "\n",
        "# Threshold probabilities to obtain predicted class labels\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "true_negatives = conf_matrix[0][0]\n",
        "false_positives = conf_matrix[0][1]\n",
        "false_negatives = conf_matrix[1][0]\n",
        "true_positives = conf_matrix[1][1]\n",
        "\n",
        "# Calculate accuracy, detection rate (sensitivity), and true negative rate (TNR)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_rate = true_positives / (true_positives + false_negatives)\n",
        "tnr = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yNfTm07N0Osa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6719c2-46c4-43c6-ca7c-52a6c1c826a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [11:36<00:00,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step\n",
            "Accuracy: 0.803921568627451\n",
            "Detection Rate (Sensitivity): 0.1\n",
            "True Negative Rate (TNR): 0.975609756097561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "cWpYvB4UD9aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for RNN input (assuming input shape is (number of samples, number of features))\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Define Vanilla RNN architecture\n",
        "model = Sequential([\n",
        "    SimpleRNN(units=32, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with loading animation\n",
        "num_epochs = 10\n",
        "with tqdm(total=num_epochs) as pbar:  # Initialize tqdm with total number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=0)\n",
        "        pbar.update(1)  # Update tqdm progress bar after each epoch\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "\n",
        "# Threshold probabilities to obtain predicted class labels\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "true_negatives = conf_matrix[0][0]\n",
        "false_positives = conf_matrix[0][1]\n",
        "false_negatives = conf_matrix[1][0]\n",
        "true_positives = conf_matrix[1][1]\n",
        "\n",
        "# Calculate accuracy, detection rate (sensitivity), and true negative rate (TNR)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_rate = true_positives / (true_positives + false_negatives)\n",
        "tnr = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SucJqRaoPalE",
        "outputId": "76a395ac-5a28-42c0-bb84-73c6d67d2485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:34<00:00, 15.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step\n",
            "Accuracy: 0.8431372549019608\n",
            "Detection Rate (Sensitivity): 0.2\n",
            "True Negative Rate (TNR): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "c3feKY_IEBzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM input (assuming input shape is (number of samples, number of time steps, number of features))\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Define LSTM architecture\n",
        "model = Sequential([\n",
        "    LSTM(units=32, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with loading animation\n",
        "num_epochs = 10\n",
        "with tqdm(total=num_epochs) as pbar:  # Initialize tqdm with total number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=0)\n",
        "        pbar.update(1)  # Update tqdm progress bar after each epoch\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "\n",
        "# Threshold probabilities to obtain predicted class labels\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "true_negatives = conf_matrix[0][0]\n",
        "false_positives = conf_matrix[0][1]\n",
        "false_negatives = conf_matrix[1][0]\n",
        "true_positives = conf_matrix[1][1]\n",
        "\n",
        "# Calculate accuracy, detection rate (sensitivity), and true negative rate (TNR)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_rate = true_positives / (true_positives + false_negatives)\n",
        "tnr = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Ll8WjqOL3a",
        "outputId": "ae3fa6cf-e5ac-4c6f-adf1-539d78b6a6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:28<00:00, 14.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step\n",
            "Accuracy: 0.8431372549019608\n",
            "Detection Rate (Sensitivity): 0.2\n",
            "True Negative Rate (TNR): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYBRID"
      ],
      "metadata": {
        "id": "rbLElQ8D_011"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/PC3.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for CNN input (assuming input shape is (number of samples, number of features, 1))\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Define CNN architecture\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    LSTM(64, return_sequences=True),  # RNN layer to capture temporal dependencies\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with loading animation\n",
        "num_epochs = 100\n",
        "with tqdm(total=num_epochs) as pbar:  # Initialize tqdm with total number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit(X_train_reshaped, y_train, epochs=1, batch_size=10, validation_split=0.2, verbose=0)\n",
        "        pbar.update(1)  # Update tqdm progress bar after each epoch\n",
        "\n",
        "# Evaluate model\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "\n",
        "# Threshold probabilities to obtain predicted class labels\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "true_negatives = conf_matrix[0][0]\n",
        "false_positives = conf_matrix[0][1]\n",
        "false_negatives = conf_matrix[1][0]\n",
        "true_positives = conf_matrix[1][1]\n",
        "\n",
        "# Calculate accuracy, detection rate (sensitivity), and true negative rate (TNR)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_rate = true_positives / (true_positives + false_negatives)\n",
        "tnr = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgf0Ew2m_j1N",
        "outputId": "7a3e5c4b-9ee0-4dca-c713-7d46f27297d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:09<00:00,  1.30s/it]\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f01ced15f30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 1s 6ms/step\n",
            "Accuracy: 0.8240740740740741\n",
            "Detection Rate (Sensitivity): 0.22857142857142856\n",
            "True Negative Rate (TNR): 0.9392265193370166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "iYdgSXkYB0pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for GRU input (assuming input shape is (number of samples, number of time steps, number of features))\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Define GRU architecture\n",
        "model = Sequential([\n",
        "    GRU(units=32, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with loading animation\n",
        "num_epochs = 10\n",
        "with tqdm(total=num_epochs) as pbar:  # Initialize tqdm with total number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=0)\n",
        "        pbar.update(1)  # Update tqdm progress bar after each epoch\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "\n",
        "# Threshold probabilities to obtain predicted class labels\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "true_negatives = conf_matrix[0][0]\n",
        "false_positives = conf_matrix[0][1]\n",
        "false_negatives = conf_matrix[1][0]\n",
        "true_positives = conf_matrix[1][1]\n",
        "\n",
        "# Calculate accuracy, detection rate (sensitivity), and true negative rate (TNR)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_rate = true_positives / (true_positives + false_negatives)\n",
        "tnr = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Detection Rate (Sensitivity):\", detection_rate)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9twzq7I7B18Q",
        "outputId": "d5091ebd-bd47-48a4-b780-b9392ee00f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:52<00:00, 17.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 10ms/step\n",
            "Accuracy: 0.803921568627451\n",
            "Detection Rate (Sensitivity): 0.1\n",
            "True Negative Rate (TNR): 0.975609756097561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN"
      ],
      "metadata": {
        "id": "3IUxK09cRzD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tqdm import tqdm  # Import tqdm for loading animation\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/MW1.arff')  # Replace 'CM1.arff' with your ARFF file path\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Defective']).values  # Features (all columns except the 'defective' column)\n",
        "y = df['Defective'].values  # Labels (the 'class' column)\n",
        "\n",
        "# Map 'Y' and 'N' labels to numerical values (e.g., 1 for 'Y' and 0 for 'N')\n",
        "y = np.where(y == b'Y', 1, 0)\n",
        "\n",
        "# Selecting only the desired columns from the features DataFrame\n",
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "X_selected = df[selected_columns].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator(latent_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=latent_dim))\n",
        "    model.add(Dense(output_dim, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the combined generator and discriminator model (GAN)\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Define the size of the random noise vector (latent space)\n",
        "latent_dim = 10\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(X_train_scaled.shape[1])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(latent_dim, X_train_scaled.shape[1])\n",
        "\n",
        "# Build and compile the GAN model\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Train the GAN\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Generate fake samples using the generator\n",
        "    gen_samples = generator.predict(noise)\n",
        "\n",
        "    # Combine real and fake samples\n",
        "    X_combined = np.concatenate((X_train_scaled, gen_samples))\n",
        "\n",
        "    # Assign labels for real and fake samples\n",
        "    y_combined = np.concatenate((np.ones((len(X_train_scaled), 1)), np.zeros((batch_size, 1))))\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss = discriminator.train_on_batch(X_combined, y_combined)\n",
        "\n",
        "    # Train the generator (via the GAN model)\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    y_gen = np.ones((batch_size, 1))\n",
        "    g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "# Evaluate the discriminator on the test set\n",
        "y_pred_prob = discriminator.predict(X_test_scaled)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy and confusion matrix\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ykc_EIQRyZf",
        "outputId": "b2ad157d-0e34-4821-d648-d991474b2f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 97ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [00:01<03:03,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [00:01<01:21,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [00:02<00:48,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [00:02<00:33,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [00:02<00:25,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [00:02<00:20,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [00:02<00:17,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [00:02<00:16,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [00:02<00:14,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [00:02<00:13,  6.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [00:03<00:12,  7.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [00:03<00:12,  7.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [00:03<00:11,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [00:03<00:09,  8.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [00:03<00:09,  8.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 18/100 [00:03<00:08,  9.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 19/100 [00:03<00:08,  9.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 20/100 [00:04<00:08,  9.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 21/100 [00:04<00:08,  9.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [00:04<00:08,  9.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 23/100 [00:04<00:08,  9.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [00:04<00:08,  9.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 25/100 [00:04<00:08,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 26/100 [00:04<00:09,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 27/100 [00:04<00:08,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 28/100 [00:04<00:08,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 29/100 [00:05<00:08,  8.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [00:05<00:08,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 31/100 [00:05<00:08,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 32/100 [00:05<00:08,  8.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 33/100 [00:05<00:08,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 34/100 [00:05<00:08,  8.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 35/100 [00:05<00:08,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 36/100 [00:05<00:07,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [00:06<00:07,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 38/100 [00:06<00:07,  8.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 39/100 [00:06<00:07,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 40/100 [00:06<00:07,  8.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 41/100 [00:06<00:07,  8.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 42/100 [00:06<00:06,  8.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 43/100 [00:06<00:06,  8.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 44/100 [00:06<00:06,  9.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [00:06<00:06,  9.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 46/100 [00:07<00:05,  9.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 47/100 [00:07<00:05,  9.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 48/100 [00:07<00:05,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 49/100 [00:07<00:05,  9.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [00:07<00:05,  9.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 52/100 [00:07<00:05,  9.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 53/100 [00:07<00:05,  8.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 54/100 [00:07<00:04,  9.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 55/100 [00:08<00:04,  9.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 57/100 [00:08<00:04,  9.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 58/100 [00:08<00:04,  9.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 59/100 [00:08<00:04,  9.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 60/100 [00:08<00:04,  9.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 61/100 [00:08<00:04,  9.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 62/100 [00:08<00:04,  8.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 63/100 [00:08<00:04,  8.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 64/100 [00:09<00:04,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 65/100 [00:09<00:04,  8.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 66/100 [00:09<00:04,  8.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 67/100 [00:09<00:04,  8.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 68/100 [00:09<00:03,  8.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 69/100 [00:09<00:03,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 70/100 [00:09<00:03,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 71/100 [00:09<00:03,  8.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 72/100 [00:10<00:03,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 73/100 [00:10<00:03,  8.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 74/100 [00:10<00:02,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 75/100 [00:10<00:02,  9.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 76/100 [00:10<00:02,  8.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 77/100 [00:10<00:02,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 78/100 [00:10<00:02,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 79/100 [00:10<00:02,  7.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 80/100 [00:10<00:02,  7.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 81/100 [00:11<00:02,  6.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 82/100 [00:11<00:02,  6.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 83/100 [00:11<00:02,  6.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 84/100 [00:11<00:02,  6.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 85/100 [00:11<00:02,  6.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 86/100 [00:11<00:02,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 87/100 [00:12<00:02,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 88/100 [00:12<00:02,  5.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 89/100 [00:12<00:01,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 90/100 [00:12<00:01,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 91/100 [00:12<00:01,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 92/100 [00:13<00:01,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 93/100 [00:13<00:01,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 94/100 [00:13<00:01,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 95/100 [00:13<00:00,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 96/100 [00:13<00:00,  5.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 97/100 [00:13<00:00,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 98/100 [00:14<00:00,  6.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 99/100 [00:14<00:00,  6.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step\n",
            "Accuracy: 0.19607843137254902\n",
            "Confusion Matrix:\n",
            "[[ 0 41]\n",
            " [ 0 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "FtDafS08pwSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "\n",
        "# Load ARFF file\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/nasa/PC5.arff')  # Replace 'your_arff_file.arff' with the path to your ARFF file\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.shape\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('output.csv', index=False)  # Replace 'output.csv' with the desired output file path\n"
      ],
      "metadata": {
        "id": "q3_cJV6QpESZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data using pandas\n",
        "data = pd.read_csv('/content/output.csv')\n",
        "data['text'] = data[['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']].apply(lambda x: ' '.join(map(str, x)), axis=1)\n",
        "# Convert numerical data to text format\n",
        "# For example, concatenate numerical values into strings\n",
        "# Map True to 1 and False to 0\n",
        "# Map 'Y' to 1 and 'N' to 0\n",
        "data['Defective'] = data['Defective'].map({\"b'Y'\": 1, \"b'N'\": 0})\n",
        "\n",
        "\n",
        "\n",
        "# Extract features and labels\n",
        "X = data['text'].tolist()  # Text inputs\n",
        "y = data['Defective'].tolist()  # Binary labels\n",
        "\n",
        "# Tokenize inputs using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "encoded_inputs = tokenizer(X, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "labels = torch.tensor(y)\n",
        "\n",
        "# Train-test split\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(encoded_inputs['input_ids'], labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Train the model\n",
        "# (Same training loop as in the previous example)\n",
        "\n",
        "# Evaluation\n",
        "# (Same evaluation procedure as in the previous example)\n"
      ],
      "metadata": {
        "id": "kbiX9XiYk4GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14187184-8990-455e-df18-416754650351"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "# Set the device (GPU or CPU)\n",
        "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Set the model in training mode\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        # Move batch to the device\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from the DataLoader\n",
        "        b_input_ids, b_labels = batch\n",
        "        # Clear any previously calculated gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, labels=b_labels)\n",
        "        # Get the loss\n",
        "        loss = outputs.loss\n",
        "        # Perform a backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Accumulate the loss\n",
        "        total_loss += loss.item()\n",
        "    # Calculate the average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEU6MU-yNqTO",
        "outputId": "a9ebb892-114c-4384-e171-82b29cf80a24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 0.5655\n",
            "Epoch 2/10, Average Loss: 0.5337\n",
            "Epoch 3/10, Average Loss: 0.5353\n",
            "Epoch 4/10, Average Loss: 0.5250\n",
            "Epoch 5/10, Average Loss: 0.5060\n",
            "Epoch 6/10, Average Loss: 0.4966\n",
            "Epoch 7/10, Average Loss: 0.4475\n",
            "Epoch 8/10, Average Loss: 0.4332\n",
            "Epoch 9/10, Average Loss: 0.3875\n",
            "Epoch 10/10, Average Loss: 0.3603\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "\n",
        "# Load ARFF file\n",
        "data, meta = arff.loadarff('CM1.arff')  # Replace 'your_arff_file.arff' with the path to your ARFF file\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.shape\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('output.csv', index=False)  # Replace 'output.csv' with the desired output file path\n",
        "print(df)"
      ],
      "metadata": {
        "id": "79pJ3h4oLVXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load data from ARFF file\n",
        "data, meta = arff.loadarff('CM1.arff')  # Replace 'your_data.arff' with your ARFF file path\n",
        "print(meta)"
      ],
      "metadata": {
        "id": "KGAA0YRYVVNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "df = df[selected_columns]\n",
        "print(new_df)"
      ],
      "metadata": {
        "id": "U0dKA-HbUdzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXQKzz8cmOKl",
        "outputId": "9a4706ad-6130-4703-e00d-c2f9cac11b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.io import arff\n",
        "\n",
        "# Directory containing the ARFF files\n",
        "directory = '/content/drive/MyDrive/nasa'\n",
        "\n",
        "# Attributes to search for\n",
        "attributes_to_search = ['LOC_EXECUTABLE','CYCLOMATIC_COMPLEXITY','CYCLOMATIC_DENSITY', 'ESSENTIAL_COMPLEXITY', 'DESIGN_COMPLEXITY','DESIGN_COMPLEXITY','HALSTEAD_LENGTH','HALSTEAD_DIFFICULTY','HALSTEAD_LEVEL','HALSTEAD_EFFORT','HALSTEAD_ERROR_EST','HALSTEAD_CONTENT','HALSTEAD_PROG_TIME','LOC_COMMENTS','LOC_BLANK','LOC_CODE_AND_COMMENT','NUM_UNIQUE_OPERATORS','NUM_UNIQUE_OPERANDS','NUM_OPERATORS','NUM_OPERANDS','BRANCH_COUNT']\n",
        "\n",
        "# List to store ARFF files containing the specified attributes\n",
        "files_with_attributes = []\n",
        "\n",
        "# Iterate over each file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.arff'):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        # Read the ARFF file\n",
        "        data, meta = arff.loadarff(filepath)\n",
        "        # Get attribute names from meta\n",
        "        attributes_in_file = meta.names()\n",
        "        # Check if all attributes_to_search are present in the file\n",
        "        if all(attr in attributes_in_file for attr in attributes_to_search):\n",
        "            files_with_attributes.append(filename)\n",
        "\n",
        "# Print the ARFF files containing the specified attributes\n",
        "for file in files_with_attributes:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waiabjPdmado",
        "outputId": "099abde7-ac17-4e03-b97f-5da193a11270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC5.arff\n",
            "MW1.arff\n",
            "MC1.arff\n",
            "PC4.arff\n",
            "MC2.arff\n",
            "PC1.arff\n",
            "CM1.arff\n",
            "PC3.arff\n",
            "KC3.arff\n"
          ]
        }
      ]
    }
  ]
}